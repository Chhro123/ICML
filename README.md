# PANDA: Phased Anomaly Diffusion with Progressive Mask Alignment for Anomaly Segmentation

## ğŸ“Œ Introduction
**PANDA** (**Phased Anomaly Diffusion with Progressive Mask Alignment**) is a novel **anomaly synthesis framework** designed for **anomaly segmentation** tasks. It overcomes key limitations of existing Anomaly Synthesis (AS) methods by generating **high-fidelity, well-aligned, and fine-grained** synthetic anomalies, significantly enhancing anomaly segmentation performance.

### ğŸ”¹ Key Challenges in Anomaly Synthesis:
- **Lack of fine-grained texture** â€“ Existing methods struggle to generate realistic anomaly textures.
- **Poor pixel alignment** â€“ Many generative methods fail to properly align synthetic anomalies with the background.
- **Ignoring small anomalies** â€“ Small, fine-grained anomalies are often overlooked during synthesis.

### âœ… How PANDA Solves These Problems:
1. **Phased Diffusion Sampling:** Injects real background information into the denoising process, ensuring realistic texture synthesis.
2. **Progressive Mask Alignment (PMA):** Smooths anomaly-background transitions, preventing misalignment artifacts.
3. **Dual-Branch Anomaly Synthesis:** Ensures small anomalies are effectively generated by integrating a dedicated **anomaly-only branch**.

PANDA achieves **state-of-the-art** (SOTA) performance on **MVTec-AD** and **BTAD** datasets, surpassing existing AS methods.

---

## ğŸ“„ Paper
If you find this work useful, please cite:

```bibtex
@inproceedings{PANDA2025,
  author = {Anonymous Authors},
  title = {PANDA: Phased Anomaly Diffusion with Progressive Mask Alignment for Anomaly Segmentation},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2025}
}
```

---

## ğŸ—ï¸ Repository Structure
```
â”œâ”€â”€ assets/                 # Figures and visualizations
â”œâ”€â”€ configs/                # Model configuration files
â”œâ”€â”€ datasets/               # Data preprocessing scripts
â”œâ”€â”€ models/                 # PANDA model implementation
â”œâ”€â”€ scripts/                # Training and inference scripts
â”œâ”€â”€ utils/                  # Utility functions
â”œâ”€â”€ main.py                 # Entry point for training and testing
â”œâ”€â”€ requirements.txt        # Required Python packages
â”œâ”€â”€ README.md               # This file
```

---

## ğŸ“¦ Installation
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-repo/PANDA.git
cd PANDA
```

### 2ï¸âƒ£ Install Dependencies
It is recommended to use a **virtual environment** (e.g., `venv` or `conda`).

```bash
pip install -r requirements.txt
```
OR (for Conda users)
```bash
conda create --name panda_env python=3.9
conda activate panda_env
pip install -r requirements.txt
```

### 3ï¸âƒ£ Setup Datasets
Download and preprocess the datasets (MVTec-AD, BTAD):

```bash
bash scripts/download_datasets.sh
```

If you want to use your own dataset, place images in:
```
datasets/
â”œâ”€â”€ my_dataset/
â”‚   â”œâ”€â”€ train/  # Normal images
â”‚   â”œâ”€â”€ test/   # Normal + Anomaly images
â”‚   â”œâ”€â”€ masks/  # Ground truth anomaly masks
```

---

## ğŸš€ Training PANDA
To train PANDA on MVTec-AD:

```bash
python main.py --dataset mvtec --epochs 100 --batch_size 16
```
For BTAD dataset:
```bash
python main.py --dataset btad --epochs 100 --batch_size 16
```

âœ… Training arguments:
| Argument        | Description                                   | Default |
|----------------|-----------------------------------------------|---------|
| `--dataset`    | Dataset name (`mvtec`, `btad`, `custom`)     | `mvtec` |
| `--epochs`     | Number of training epochs                    | `100`   |
| `--batch_size` | Batch size                                   | `16`    |
| `--lr`         | Learning rate                                | `1e-4`  |

---

## ğŸ§ Evaluation & Inference
Run inference on test images:

```bash
python main.py --mode test --dataset mvtec
```
OR for custom images:
```bash
python main.py --mode test --dataset custom --input_dir path/to/images
```

Results will be saved in `outputs/` directory.

---


## ğŸ“¢ Acknowledgments
This work is built upon **Denoising Diffusion Models** and **Latent Diffusion Models (LDMs)**. We acknowledge contributions from prior work in **anomaly detection and segmentation**.

For questions or contributions, feel free to open an **Issue** or submit a **Pull Request**.

---

## ğŸ“œ License
This project is released under the **MIT License**.

---
